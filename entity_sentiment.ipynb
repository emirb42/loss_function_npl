{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification\n",
        "import torch.nn.functional as F\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/MyDrive')\n",
        "\n",
        "# Cihazı ayarla\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Entity recognition model\n",
        "ner_tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-large-finetuned-conll03-english\")\n",
        "ner_model = AutoModelForTokenClassification.from_pretrained(\"FacebookAI/xlm-roberta-large-finetuned-conll03-english\").to(device)\n",
        "\n",
        "# Sentiment analysis model\n",
        "sa_model = AutoModelForSequenceClassification.from_pretrained(\"savasy/bert-base-turkish-sentiment-cased\").to(device)\n",
        "sa_tokenizer = AutoTokenizer.from_pretrained(\"savasy/bert-base-turkish-sentiment-cased\")\n",
        "\n",
        "# Sentiment analysis pipeline\n",
        "sa = pipeline(\"sentiment-analysis\", tokenizer=sa_tokenizer, model=sa_model, device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    # Metni tokenize edip aynı cihaza taşıyoruz\n",
        "    inputs = sa_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = sa_model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    score, predicted_label = torch.max(probabilities, dim=-1)\n",
        "\n",
        "    sentiment = \"olumlu\" if predicted_label.item() == 1 else \"olumsuz\"\n",
        "    return sentiment, score.item()\n",
        "\n",
        "def extract_entities(text):\n",
        "    inputs = ner_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = ner_model(**inputs)\n",
        "    predictions = torch.argmax(outputs.logits, dim=2).squeeze().tolist()\n",
        "    tokens = ner_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
        "\n",
        "    entities = {'PER': [], 'ORG': [], 'LOC': [], 'MISC': []}\n",
        "    current_entity = \"\"\n",
        "    current_type = \"\"\n",
        "\n",
        "    for token, pred in zip(tokens, predictions):\n",
        "        label = ner_model.config.id2label[pred]\n",
        "\n",
        "        # Gereksiz tokenleri filtreleme\n",
        "        if token in ['</s>', '<pad>', '<s>', '.com', '.net', '.org']:\n",
        "            continue\n",
        "\n",
        "        # Subword tokens are usually prefixed with \"▁\", remove this prefix\n",
        "        token = token.replace('▁', '')\n",
        "\n",
        "        if label.startswith('B-') or label.startswith('I-'):\n",
        "            entity_type = label.split('-')[1]\n",
        "            if label.startswith('B-') and current_entity:\n",
        "                # Aynı entity'yi iki kere eklememek için kontrol\n",
        "                if current_entity.strip() not in entities[current_type]:\n",
        "                    entities[current_type].append(current_entity.strip())\n",
        "                current_entity = \"\"\n",
        "            current_entity += token + \"\"\n",
        "            current_type = entity_type\n",
        "        elif current_entity:\n",
        "            # Aynı entity'yi iki kere eklememek için kontrol\n",
        "            if current_entity.strip() not in entities[current_type]:\n",
        "                entities[current_type].append(current_entity.strip())\n",
        "            current_entity = \"\"\n",
        "            current_type = \"\"\n",
        "\n",
        "    if current_entity and current_entity.strip() not in entities[current_type]:\n",
        "        entities[current_type].append(current_entity.strip())\n",
        "\n",
        "    # \".com\", \".net\" gibi uzantılarla biten entity'leri birleştirme\n",
        "    for key in entities:\n",
        "        merged_entities = []\n",
        "        i = 0\n",
        "        while i < len(entities[key]):\n",
        "            if i < len(entities[key]) - 1 and entities[key][i+1] in ['com', 'net', 'org']:\n",
        "                merged_entities.append(entities[key][i] + '.' + entities[key][i+1])\n",
        "                i += 2\n",
        "            else:\n",
        "                merged_entities.append(entities[key][i])\n",
        "                i += 1\n",
        "        entities[key] = list(set(merged_entities))  # Aynı entity'yi iki kez yazmamak için set kullanımı\n",
        "\n",
        "    return entities\n",
        "\n",
        "\n",
        "def get_window(text, entity, window_size=50):\n",
        "    words = text.split()\n",
        "    entity_words = entity.split()\n",
        "    for i in range(len(words) - len(entity_words) + 1):\n",
        "        if words[i:i+len(entity_words)] == entity_words:\n",
        "            entity_index = i + len(entity_words) - 1\n",
        "            start = max(0, entity_index - window_size)\n",
        "            end = min(len(words), entity_index + window_size + 1)\n",
        "            return ' '.join(words[start:end])\n",
        "    return text\n",
        "\n",
        "def analyze_text_method1(text, actual_label):\n",
        "    entities = extract_entities(text)\n",
        "    overall_sentiment, overall_score = analyze_sentiment(text)\n",
        "\n",
        "    results = []\n",
        "    for entity_type, entity_list in entities.items():\n",
        "        for entity in entity_list:\n",
        "            predicted_sentiment = \"olumlu\" if actual_label == 'olumlu' else \"olumsuz\"\n",
        "            results.append({\n",
        "                \"entity\": entity,\n",
        "                \"type\": entity_type,\n",
        "                \"sentiment\": predicted_sentiment,\n",
        "                \"score\": overall_score\n",
        "            })\n",
        "\n",
        "    return {\n",
        "        \"entity_list\": entities,\n",
        "        \"results\": results\n",
        "    }\n",
        "\n",
        "def analyze_text_method2(text, actual_label):\n",
        "    entities = extract_entities(text)\n",
        "\n",
        "    results = []\n",
        "    for entity_type, entity_list in entities.items():\n",
        "        for entity in entity_list:\n",
        "            window = get_window(text, entity)\n",
        "            predicted_sentiment = \"olumlu\" if actual_label == 'olumlu' else \"olumsuz\"\n",
        "            sentiment, score = analyze_sentiment(window)\n",
        "            results.append({\n",
        "                \"entity\": entity,\n",
        "                \"type\": entity_type,\n",
        "                \"sentiment\": predicted_sentiment,\n",
        "                \"score\": score\n",
        "            })\n",
        "\n",
        "    return {\n",
        "        \"entity_list\": entities,\n",
        "        \"results\": results\n",
        "    }\n",
        "\n",
        "def analyze_text_method3(text, actual_label):\n",
        "    entities = extract_entities(text)\n",
        "    overall_sentiment, overall_score = analyze_sentiment(text)\n",
        "\n",
        "    results = []\n",
        "    for entity_type, entity_list in entities.items():\n",
        "        for entity in entity_list:\n",
        "            sentences = text.split('.')\n",
        "            entity_sentence = next((s for s in sentences if entity in s), text)\n",
        "            sentence_sentiment, sentence_score = analyze_sentiment(entity_sentence)\n",
        "\n",
        "            weighted_score = 0.7 * sentence_score + 0.3 * overall_score\n",
        "            weighted_sentiment = \"olumlu\" if weighted_score > 0.5 else \"olumsuz\"\n",
        "            predicted_sentiment = \"olumlu\" if actual_label == 'olumlu' else \"olumsuz\"\n",
        "\n",
        "            results.append({\n",
        "                \"entity\": entity,\n",
        "                \"type\": entity_type,\n",
        "                \"sentiment\": predicted_sentiment,\n",
        "                \"score\": weighted_score,\n",
        "                \"sentence_sentiment\": sentence_sentiment,\n",
        "                \"overall_sentiment\": overall_sentiment\n",
        "            })\n",
        "\n",
        "    return {\n",
        "        \"entity_list\": entities,\n",
        "        \"results\": results\n",
        "    }\n",
        "\n",
        "\n",
        "def process_csv(input_csv, output_csv):\n",
        "    df = pd.read_csv(input_csv)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        text = row['text'].lower()\n",
        "        actual_label = row['label']\n",
        "\n",
        "        result1 = analyze_text_method1(text, actual_label)\n",
        "\n",
        "        if not any(result1['entity_list'].values()):\n",
        "            continue\n",
        "\n",
        "        # Entity list'i ve sonuçları formatlıyoruz\n",
        "        entity_list_str = str([entity for entities in result1['entity_list'].values() for entity in entities])\n",
        "        results_str = str([{\"entity\": result['entity'], \"sentiment\": result['sentiment']} for result in result1['results']])\n",
        "\n",
        "        results.append({\n",
        "            \"input_text\": text,\n",
        "            \"entity_list\": entity_list_str,\n",
        "            \"results\": results_str\n",
        "        })\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv(output_csv, index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dosya = \"veri400klabelkisa\"\n",
        "    input_csv = f'/content/MyDrive/MyDrive/dilsileme/{dosya}.csv'\n",
        "    output_csv = f'/content/MyDrive/MyDrive/dilsileme/{dosya}_results.csv'\n",
        "    process_csv(input_csv, output_csv)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CelZ4NTtMRgR",
        "outputId": "34b27c37-fc1c-49a4-8147-dce3b40a3d99"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/MyDrive; to attempt to forcibly remount, call drive.mount(\"/content/MyDrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at FacebookAI/xlm-roberta-large-finetuned-conll03-english were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U16tZVkZLxfo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}